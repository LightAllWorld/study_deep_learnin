{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:     0\tLoss: 14.204321\tAcc: 0.000000%\n",
      "Step:   200\tLoss: 0.001316\tAcc: 50.000000%\n",
      "Step:   400\tLoss: 0.000544\tAcc: 50.000000%\n",
      "Step:   600\tLoss: 0.000344\tAcc: 50.000000%\n",
      "Step:   800\tLoss: 0.000252\tAcc: 50.000000%\n",
      "Step:  1000\tLoss: 0.000200\tAcc: 50.000000%\n",
      "Step:  1200\tLoss: 0.000165\tAcc: 50.000000%\n",
      "Step:  1400\tLoss: 0.000141\tAcc: 50.000000%\n",
      "Step:  1600\tLoss: 0.000123\tAcc: 50.000000%\n",
      "Step:  1800\tLoss: 0.000110\tAcc: 50.000000%\n",
      "Step:  2000\tLoss: 0.000099\tAcc: 50.000000%\n",
      "[False] predication: 4 True Y: 5\n",
      "[True] predication: 4 True Y: 4\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "XY = np.loadtxt(\"fancy_data.csv\",delimiter = \" \", dtype = np.float32)\n",
    "x_data = XY[:,0:-1]\n",
    "y_data = XY[:,[-1]]\n",
    "\n",
    "nb_classes = 5 # Y의 값은 0~4 사이의 정수값을 갖음으로 총 7개의 값으로 분류가능\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 5])\n",
    "Y = tf.placeholder(tf.int32, [None, 1]) # Y값은 0~6까지 중 1개의 값을 가짐\n",
    "\n",
    "Y_one_hot = tf.one_hot(Y, nb_classes) # one-hot 자체가 n차원의 데이터가 들어오면, n+1차원의 값으로 반환\n",
    "Y_one_hot = tf.reshape(Y_one_hot, [-1, nb_classes]) # N+1차원의 값을 원하는 모양으로 바꾸기 위한 작업\n",
    "\n",
    "W = tf.Variable(tf.random_normal([5, nb_classes]), name = \"weight\") # x에는 5개의 attribute가 존재하고, label로 7개의 값중 하나가 도출\n",
    "b = tf.Variable(tf.random_normal([nb_classes]), name = \"bias\") # y는 7개의 값중 1개로 도출\n",
    "\n",
    "logits = tf.matmul(X, W) + b\n",
    "hypothesis = tf.nn.softmax(logits)\n",
    "\n",
    "# cost함수를 정의함에 있어서 hyphothesis가 logits이며, 그에 대한 labels은 y_one_hot 형식으로 도출의미\n",
    "# cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "cost_i = tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = Y_one_hot)\n",
    "\n",
    "cost = tf.reduce_mean(cost_i)\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.05).minimize(cost)\n",
    "\n",
    "predication = tf.argmax(hypothesis, 1)\n",
    "correct_predication = tf.equal(predication, tf.argmax(Y_one_hot, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_predication, tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "for step in range(2000+1) :\n",
    "    sess.run(optimizer, feed_dict = {X : x_data, Y : y_data})\n",
    "    if step % 200 == 0 :\n",
    "        loss, acc = sess.run([cost, accuracy], feed_dict = {X : x_data, Y : y_data})\n",
    "        print(\"Step: {:5}\\tLoss: {:3f}\\tAcc: {:2%}\".format(step, loss, acc))\n",
    "        \n",
    "pred = sess.run(predication, feed_dict = {X:x_data})\n",
    "for p, y in zip(pred, y_data.flatten()):\n",
    "    print(\"[{}] predication: {} True Y: {}\".format(p == int(y), p, int(y)))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
